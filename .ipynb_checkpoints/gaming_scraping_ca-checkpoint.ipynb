{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping prototype\n",
    "\n",
    "The following code is a prototype that scrapes data from Casino City's Gaming Directory. Their current directory informs us on the re-opening of gaming properties in the United States following the Covid-19 pandemic.\n",
    "\n",
    "https://www.gamingdirectory.com/covid-19/reopened/\n",
    "\n",
    "Notes:\n",
    "The site requires registration in order to see an entire directory listing. Otherwise, you are limited to the first 20 records or so. Since the data sits behind a login, scraping cannot be done on the entire dataset. To circumvent this, you must register, login, and save the page locally, and run the code on the local file.\n",
    "\n",
    "The scraping was challenging because of the poor structure of the data. Specifically, the `<div>` tags did not have unique identifiers, making it difficult to differentiate one field from the other. \n",
    "\n",
    "Reference: https://www.dataquest.io/blog/web-scraping-tutorial-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75f2841de853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msoupedrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mProperty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoupedrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orgCol data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mCity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoupedrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orgCol data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mPropertyType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoupedrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'statusCol data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# grab the html file on your local drive\n",
    "url = \"property_ca.html\"\n",
    "\n",
    "# soupify it\n",
    "soup = BeautifulSoup(open(url),'html.parser')\n",
    "\n",
    "# If you were to grab data from the live site, this is the syntax, but you will only get the first 20 records\n",
    "# page = requests.get(\"https://www.gamingdirectory.com/covid-19/reopened/\")\n",
    "# soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "# split the data by the div that separates each row\n",
    "split_data = soup.prettify().split('<div style=\"clear:both\">')\n",
    "\n",
    "# here are the relevant column headers\n",
    "Property = []\n",
    "City = []\n",
    "PropertyType = []\n",
    "Status = []\n",
    "DateClosed = []\n",
    "DateReopened = []\n",
    "Article = []\n",
    "ArticleLink = []\n",
    "\n",
    "# loop through the results\n",
    "counter = 0\n",
    "for row in split_data:\n",
    "#     Start grabbing from second row, as first row does not contain data\n",
    "    if counter > 1 and counter < len(split_data)-1:\n",
    "        soupedrow = BeautifulSoup(row,'html.parser')\n",
    "        Property.append(soupedrow.find_all('div',class_='orgCol data')[0].text)\n",
    "        City.append(soupedrow.find_all('div',class_='orgCol data')[0].text)\n",
    "        PropertyType.append(soupedrow.find_all('div',class_='statusCol data')[0].text)\n",
    "        Status.append(soupedrow.find_all('div',class_='statusCol data')[1].text)\n",
    "        DateClosed.append(soupedrow.find_all('div',class_='dateCol data')[1].text)\n",
    "        DateReopened.append(soupedrow.find_all('div',class_='dateCol data')[0].text)\n",
    "        Article.append(soupedrow.find_all('div',class_='articleCol data')[0].text)\n",
    "    counter = counter + 1\n",
    "    \n",
    "# get rid of \\n (new lines)\n",
    "Property = [item.strip() for item in Property]\n",
    "City = [item.strip() for item in City]\n",
    "PropertyType = [item.strip() for item in PropertyType]\n",
    "Status = [item.strip() for item in Status]\n",
    "DateClosed = [item.strip() for item in DateClosed]\n",
    "DateReopened = [item.strip() for item in DateReopened]\n",
    "Article = [item.strip() for item in Article]\n",
    "\n",
    "# put the data into a dataframe\n",
    "GamingExport = pd.DataFrame({\n",
    "    \"Property\": Property,\n",
    "    \"City\": City,\n",
    "    \"PropertyType\": PropertyType,\n",
    "    \"Status\": Status,\n",
    "    \"DateClosed\": DateClosed,\n",
    "    \"DateReopened\": DateReopened,\n",
    "\n",
    "    \"Article\": Article\n",
    "})\n",
    "\n",
    "# let's check out the data here\n",
    "GamingExport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export it as a csv\n",
    "GamingExport.to_csv('GamingExport_ca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
